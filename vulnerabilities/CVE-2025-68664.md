# CVE-2025-68664: LangChain Serialization Injection "LangGrinch" (CVSS 9.3)

## Quick Reference

| Field | Value |
|-------|-------|
| **CVE ID** | CVE-2025-68664 |
| **Framework** | LangChain Core |
| **Affected Versions** | langchain-core < 0.3.81, 1.0.0 to < 1.2.5 |
| **Fixed Version** | 0.3.81, 1.2.5 |
| **CVSS Score** | 9.3 (Critical) |
| **CWE** | CWE-502 (Deserialization of Untrusted Data) |
| **Attack Vector** | Network (via prompt injection) |
| **Disclosed** | 2025-12-04 |

## Summary

A serialization injection vulnerability in LangChain's `dumps()` and `dumpd()` functions allows attackers to inject malicious serialized objects that get executed during deserialization. Dubbed "LangGrinch," this vulnerability is particularly dangerous because it can be triggered through **prompt injection** - meaning an LLM's output can be crafted to exploit the flaw.

**The attack chain is insidious:** A malicious prompt causes the LLM to output specially crafted data that flows through LangChain's serialization pipeline, achieving code execution without any direct attacker access to the application.

## Technical Details

**Vulnerable Component:** `dumps()`, `dumpd()`, `loads()`, `load()` functions in langchain-core

**Root Cause:** LangChain uses a special `lc` key internally to mark serialized objects. The serialization functions fail to escape user-controlled dictionaries that contain this key, treating them as legitimate LangChain objects during deserialization.

**Attack Pattern:**
1. Attacker crafts a prompt that causes LLM to output data containing `lc` key structures
2. LLM response flows into fields like `additional_kwargs` or `response_metadata`
3. Application serializes and later deserializes this data (common in streaming, logging)
4. Malicious payload is treated as a LangChain object and instantiated
5. Arbitrary code execution or secret extraction occurs

**Impact Scenarios:**
- **Secret extraction:** With `secrets_from_env=True` (formerly the default), environment variables are exposed
- **Class instantiation:** Objects from trusted namespaces (langchain_core, langchain, langchain_community) can be instantiated
- **Code execution:** Via Jinja2 template injection

## Proof of Concept

The attack leverages prompt injection to create malicious serialized structures:

```python
# Simplified representation of the attack payload structure
# The attacker causes the LLM to output something like:
malicious_response = {
    "lc": 1,
    "type": "constructor",
    "id": ["langchain", "some", "class"],
    "kwargs": {"secret": {"$ref": "env:API_KEY"}}
}

# When this flows through dumps()/loads() with secrets_from_env=True,
# environment variables can be extracted
```

## Remediation

### Immediate Actions

1. **Upgrade immediately:**
   ```bash
   pip install langchain-core>=0.3.81
   # or for 1.x
   pip install langchain-core>=1.2.5
   ```

2. **If unable to upgrade:**
   - Set `secrets_from_env=False` explicitly in all `load()` calls
   - Audit any code that serializes/deserializes LLM outputs
   - Consider removing Jinja2 template support if not needed

### Patch Details

The fix introduces:
- New `allowed_objects` allowlist parameter for controlling deserialization
- `secrets_from_env` now defaults to `False`
- Jinja2 templates blocked by default

## GRC Implications

### Risk Assessment

**Risk Level:** CRITICAL - Novel attack vector via prompt injection

**Unique Risk Factors:**
- Attack can be triggered remotely via prompts, not just malicious files
- Standard prompt injection defenses may not protect against this
- Exploitation can occur through normal application workflows (streaming, logging)

**Impact Scenarios:**
- Secret exfiltration from production AI systems
- Lateral movement via extracted API keys and credentials
- Supply chain attacks if AI agents interact with other systems

### Vendor Assessment Questions

If evaluating vendors using LangChain:

1. "What version of langchain-core are you running?"
2. "Are you using the dumps/loads serialization APIs?"
3. "Is secrets_from_env enabled in your configuration?"
4. "How do you sanitize LLM outputs before serialization?"
5. "Are you aware of CVE-2025-68664 and have you patched?"

### Control Recommendations

| Control | Description | Priority |
|---------|-------------|----------|
| Upgrade LangChain | Update to 0.3.81+ or 1.2.5+ | Critical |
| Disable secrets_from_env | Set to False explicitly | Critical |
| Input validation | Don't serialize raw LLM outputs | High |
| Prompt hardening | Defense-in-depth against injection | High |
| Environment isolation | Limit env vars available to AI workloads | Medium |

### Policy Implications

This CVE demonstrates that **prompt injection can lead to code execution** - a significant escalation from typical prompt injection impacts. Consider adding:

- "LLM outputs must be treated as untrusted data and not serialized without sanitization"
- "AI framework serialization features must use restrictive defaults"
- "Secret management for AI systems must minimize environment variable exposure"

## Related Vulnerability

CVE-2025-68665 (CVSS 8.6) - Similar serialization injection in LangChain.js

## References

- [Cyata Blog: LangGrinch Analysis](https://cyata.ai/blog/langgrinch-langchain-core-cve-2025-68664/)
- [The Hacker News](https://thehackernews.com/2025/12/critical-langchain-core-vulnerability.html)
- [NVD Entry](https://nvd.nist.gov/vuln/detail/CVE-2025-68664)
- [GitHub Advisory GHSA-c67j-w6g6-q2cm](https://github.com/advisories/GHSA-c67j-w6g6-q2cm)
- [SOCRadar Analysis](https://socradar.io/blog/cve-2025-68664-langchain-flaw-secret-extraction/)

## Timeline

| Date | Event |
|------|-------|
| 2025-12-04 | Reported by Yarden Porat |
| 2025-12-XX | LangChain releases patches |
| 2025-12-XX | Public disclosure |

---

*Last updated: 2026-01-18*

*Tags: `langchain` `critical` `cvss-9` `prompt-injection` `secrets` `rce` `ai-agents`*
